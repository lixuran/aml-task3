{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Helper notebook for loading the data and saving the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lixuran/anaconda3/envs/pai_4/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import gzip\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from skimage import io\n",
    "from skimage.transform import resize\n",
    "from skimage import img_as_bool\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as T\n",
    "import torchio as tio \n",
    "from PIL import Image\n",
    "import random\n",
    "seed =0\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import time\n",
    "import os\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from torch.optim import Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine the device to be used for training and evaluation\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# initialize learning rate, number of epochs to train for, and the\n",
    "# batch size\n",
    "INIT_LR = 0.001\n",
    "NUM_EPOCHS = 20\n",
    "BATCH_SIZE = 16\n",
    "# define the path to the base output directory\n",
    "BASE_OUTPUT = \"output\"\n",
    "# define the path to the output serialized model, model training\n",
    "# plot, and testing image paths\n",
    "MODEL_PATH = os.path.join(BASE_OUTPUT, \"unet_tgs_salt.pth\")\n",
    "PLOT_PATH = os.path.sep.join([BASE_OUTPUT, \"plot.png\"])\n",
    "TEST_PATHS = os.path.sep.join([BASE_OUTPUT, \"test_paths.txt\"])\n",
    "image_size_w,image_size_h  = 128,128 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_zipped_pickle(filename):\n",
    "    with gzip.open(filename, 'rb') as f:\n",
    "        loaded_object = pickle.load(f)\n",
    "        return loaded_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_zipped_pickle(obj, filename):\n",
    "    with gzip.open(filename, 'wb') as f:\n",
    "        pickle.dump(obj, f, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data, make predictions and save prediction in correct format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "train_data = load_zipped_pickle(\"train.pkl\")\n",
    "test_data = load_zipped_pickle(\"test.pkl\")\n",
    "samples = load_zipped_pickle(\"sample.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = [] # v is video\n",
    "l = [] # l is label lol\n",
    "f = []\n",
    "b = []\n",
    "\n",
    "for i in train_data:\n",
    "    v.append(i[\"video\"])\n",
    "for i in train_data:\n",
    "    l.append(i[\"label\"])\n",
    "for i in train_data:\n",
    "    f.append(i[\"frames\"])\n",
    "\n",
    "for i in train_data:\n",
    "    b.append(i[\"box\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# tensor_x = torch.Tensor(v2) # transform to torch tensor\n",
    "# tensor_y = torch.Tensor(ls)\n",
    "\n",
    "# my_dataset = TensorDataset(tensor_x,tensor_y) # create your datset\n",
    "# my_dataloader = DataLoader(my_dataset)\n",
    "#data augmentation \n",
    "#Rotation, scaling, translation, deformation grid, maybe sheering\n",
    "\n",
    "#plt.imshow(b[0])\n",
    "bs = []\n",
    "ls = []\n",
    "for i in range(len(v)):\n",
    "    res = resize(b[i], (image_size_w,image_size_h))\n",
    "    bs.append(res)\n",
    "    for j in [0,1,2]:\n",
    "         res = resize(l[i][j], (image_size_w,image_size_h))\n",
    "         ls.append(res)\n",
    "ls = np.array(ls)\n",
    "bs = np.array(bs)\n",
    "#print(res)\n",
    "#plt.imshow(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(195, 128, 128)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ls.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "image_size_w,image_size_h  = 128,128 \n",
    "v2 = []\n",
    "v_eualised =[]\n",
    "for index, video in enumerate(v):\n",
    "    for i in f[index]:\n",
    "        \n",
    "        res = cv.resize(video[:,:,i], dsize=(image_size_w,image_size_h), interpolation=cv.INTER_CUBIC)\n",
    "        res2 = cv.equalizeHist(res)\n",
    "        #showcase = np.hstack((res,res2))\n",
    "        # cv.imwrite(\"img.png\",showcase)\n",
    "        # break\n",
    "        res = res/255\n",
    "        res = res - res.mean()\n",
    "        res2 = res2/255\n",
    "        res2 = res2 - res2.mean()\n",
    "        #print(res.mean())\n",
    "        v2.append(res) \n",
    "        v_eualised.append(res2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, xs, ys,transform,target_trans):\n",
    "        self.xs = xs\n",
    "        self.ys = ys\n",
    "        self.trans = transform\n",
    "        self.target_trans = target_trans\n",
    "    def __len__(self):\n",
    "            return self.xs.shape[0]\n",
    "    def __getitem__(self, idx):\n",
    "            y =self.ys[idx] \n",
    "            if self.target_trans != None:\n",
    "                y = self.target_trans(y)\n",
    "            x = self.xs[idx]\n",
    "            if self.trans != None:\n",
    "                x = self.trans(x)\n",
    "\n",
    "            return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 128, 128])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans(tensor_x[0].unsqueeze(0)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([195, 1, 128, 128])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_35606/3033182119.py:1: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)\n",
      "  tensor_x = torch.Tensor(v2) # transform to torch tensor\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "tensor_x = torch.Tensor(v2) # transform to torch tensor\n",
    "tensor_y = torch.Tensor(ls)\n",
    "\n",
    "tensor_x = tensor_x.unsqueeze(1)\n",
    "tensor_y = tensor_y.unsqueeze(1)\n",
    "\n",
    "trans = T.Compose([\n",
    "   T.RandomRotation(10),\n",
    "   T.RandomResizedCrop(128,scale=(0.5, 1.0)),\n",
    "   T.RandomAffine(degrees=0,translate=(0.2,0.2)),\n",
    "   #tio.RandomElasticDeformation(),\n",
    "   \n",
    "])\n",
    "#trans =None\n",
    "# could also calculate mean across all images??? just a single value...\n",
    "target_trans = trans \n",
    "my_dataset = ImageDataset(tensor_x,tensor_y,trans,target_trans) # create your datset\n",
    "train_size = int(0.9 * len(my_dataset))\n",
    "val_size = len(my_dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(my_dataset, [train_size, val_size])\n",
    "train_dataloader = DataLoader(train_dataset,shuffle=True,batch_size=BATCH_SIZE,num_workers=2)\n",
    "val_dataloader = DataLoader(val_dataset,shuffle=False,batch_size=BATCH_SIZE,num_workers=2)\n",
    "\n",
    "\n",
    "#data augmentation \n",
    "#Rotation, scaling, translation, deformation grid, maybe sheering\n",
    "# define the transform using torchvision transform. then define a custom dataset that takes a transform to init, then   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size_w,image_size_h  = 128,128 \n",
    "class Block(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_ch, out_ch, 3)\n",
    "        self.relu  = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.conv2(self.relu(self.conv1(x)))\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, chs=(1,32,64,128)):\n",
    "        super().__init__()\n",
    "        self.enc_blocks = nn.ModuleList([Block(chs[i], chs[i+1]) for i in range(len(chs)-1)])\n",
    "        self.pool       = nn.MaxPool2d(2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        ftrs = []\n",
    "        for block in self.enc_blocks:\n",
    "            x = block(x)\n",
    "            ftrs.append(x)\n",
    "            x = self.pool(x)\n",
    "        return ftrs\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, chs=(128, 64, 32)):\n",
    "        super().__init__()\n",
    "        self.chs         = chs\n",
    "        self.upconvs    = nn.ModuleList([nn.ConvTranspose2d(chs[i], chs[i+1], 2, 2) for i in range(len(chs)-1)])\n",
    "        self.dec_blocks = nn.ModuleList([Block(chs[i], chs[i+1]) for i in range(len(chs)-1)]) \n",
    "        \n",
    "    def forward(self, x, encoder_features):\n",
    "        for i in range(len(self.chs)-1):\n",
    "            x        = self.upconvs[i](x)\n",
    "            enc_ftrs = self.crop(encoder_features[i], x)\n",
    "            x        = torch.cat([x, enc_ftrs], dim=1)\n",
    "            #print(\"x shape\",x.shape)\n",
    "            x        = self.dec_blocks[i](x)\n",
    "        return x\n",
    "    \n",
    "    def crop(self, enc_ftrs, x):\n",
    "        _, _, H, W = x.shape\n",
    "        enc_ftrs   = torchvision.transforms.CenterCrop([H, W])(enc_ftrs)\n",
    "        return enc_ftrs\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, enc_chs=(1,32,64,128), dec_chs=( 128, 64,32), num_class=1, retain_dim=True, out_sz=(image_size_h,image_size_w)):\n",
    "        super().__init__()\n",
    "        self.encoder     = Encoder(enc_chs)\n",
    "        self.decoder     = Decoder(dec_chs)\n",
    "        self.head        = nn.Conv2d(dec_chs[-1], num_class, 1)\n",
    "        self.retain_dim  = retain_dim\n",
    "        self.out_sz = out_sz\n",
    "    def forward(self, x):\n",
    "        enc_ftrs = self.encoder(x)\n",
    "        #print(enc_ftrs)\n",
    "        out      = self.decoder(enc_ftrs[::-1][0], enc_ftrs[::-1][1:])\n",
    "        out      = self.head(out)\n",
    "        if self.retain_dim:\n",
    "            out = F.interpolate(out, self.out_sz)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 128, 128])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = UNet()\n",
    "x    = torch.randn(1, 1, 128, 128)\n",
    "model(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] training the network...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "  0%|                                                    | 0/20 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "#training loops\n",
    "\n",
    "# initialize our UNet model\n",
    "unet = UNet().to(DEVICE)\n",
    "# initialize loss function and optimizer\n",
    "lossFunc = BCEWithLogitsLoss()\n",
    "opt = Adam(unet.parameters(), lr=INIT_LR)\n",
    "# calculate steps per epoch for training and test set\n",
    "trainSteps = len(train_dataset) // BATCH_SIZE\n",
    "testSteps = len(val_dataset) // BATCH_SIZE\n",
    "# initialize a dictionary to store training history\n",
    "H = {\"train_loss\": [], \"test_loss\": []}\n",
    "\n",
    "# loop over epochs\n",
    "print(\"[INFO] training the network...\")\n",
    "startTime = time.time()\n",
    "for e in tqdm(range(NUM_EPOCHS)):\n",
    "    # set the model in training mode\n",
    "    unet.train()\n",
    "    # initialize the total training and validation loss\n",
    "    totalTrainLoss = 0\n",
    "    totalTestLoss = 0\n",
    "    # loop over the training set\n",
    "    for (i, (x, y)) in enumerate(train_dataloader):\n",
    "        # send the input to the device\n",
    "        (x, y) = (x.to(DEVICE), y.to(DEVICE))\n",
    "        # perform a forward pass and calculate the training loss\n",
    "        pred = unet(x)\n",
    "        loss = lossFunc(pred, y)\n",
    "        # first, zero out any previously accumulated gradients, then\n",
    "        # perform backpropagation, and then update model parameters\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        # add the loss to the total training loss so far\n",
    "        totalTrainLoss += loss\n",
    "    # switch off autograd\n",
    "    with torch.no_grad():\n",
    "        # set the model in evaluation mode\n",
    "        unet.eval()\n",
    "        # loop over the validation set\n",
    "        for (x, y) in val_dataloader:\n",
    "        # send the input to the device\n",
    "            (x, y) = (x.to(DEVICE), y.to(DEVICE))\n",
    "            # make the predictions and calculate the validation loss\n",
    "            pred = unet(x)\n",
    "            totalTestLoss += lossFunc(pred, y)\n",
    "    # calculate the average training and validation loss\n",
    "    avgTrainLoss = totalTrainLoss / trainSteps\n",
    "    avgTestLoss = totalTestLoss / testSteps\n",
    "    #update our training history\n",
    "    H[\"train_loss\"].append(avgTrainLoss.cpu().detach().numpy())\n",
    "    H[\"test_loss\"].append(avgTestLoss.cpu().detach().numpy())\n",
    "    # print the model training and validation information\n",
    "    print(\"[INFO] EPOCH: {}/{}\".format(e + 1, NUM_EPOCHS))\n",
    "    print(\"Train loss: {:.6f}, Test loss: {:.4f}\".format(\n",
    "    avgTrainLoss, avgTestLoss))\n",
    "# display the total time needed to perform the training\n",
    "endTime = time.time()\n",
    "print(\"[INFO] total time taken to train the model: {:.2f}s\".format(\n",
    "endTime - startTime))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the training loss\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(H[\"train_loss\"], label=\"train_loss\")\n",
    "plt.plot(H[\"test_loss\"], label=\"test_loss\")\n",
    "plt.title(\"Training Loss on Dataset\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.savefig(PLOT_PATH)\n",
    "# serialize the model to disk\n",
    "torch.save(unet, MODEL_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualization\n",
    "index = 48\n",
    "for index in range(45,60):\n",
    "    print(index)\n",
    "    for i, index2 in enumerate(f[index]):\n",
    "        plt.figure()\n",
    "        plt.subplot(1,3,1)\n",
    "        plt.imshow(l[index][:,:,index2], cmap='gray')\n",
    "        plt.subplot(1,3,2)\n",
    "        plt.imshow(l[index][:,:,index2], cmap='gray')\n",
    "        image_mask = v[index][:,:,f[index][i]]\n",
    "        plt.imshow(image_mask, cmap='jet', alpha=0.5)\n",
    "        plt.subplot(1,3,3)\n",
    "        plt.imshow(l[index][:,:,index2], cmap='gray')\n",
    "        image_mask = b[index]\n",
    "        plt.imshow(image_mask, cmap='jet', alpha=0.5)\n",
    "        plt.show()\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3994/1725030012.py:2: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  l =np.array(l) /255\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    }
   ],
   "source": [
    "#data standardization normalization and histogram correction??? est:1h\n",
    "l =np.array(l) /255\n",
    "l.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data augmentation est: 1h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model building unet: est: 1h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training loop, validation, loss and metric and visualization 1h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing loop metric and visualization and store result 1h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model structure , loss , and hyper parameter selection 2h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# buffer time 2h "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optional model: optical flow \n",
    "# mask r cnn? for roi prediction?\n",
    "# unsupervised low rank disentangle ? 4h\n",
    "\n",
    "#finalise and submission 2h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(l[50].shape[2]):\n",
    "    plt.figure()\n",
    "    plt.imshow(l[50][:,:,i], cmap='gray')\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.array(l)=='amateur')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(112, 112, 334)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video = np.array(train_data[0]['video'])\n",
    "video.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1232\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(112, 112)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "box= np.array(train_data[0]['box'])\n",
    "print(np.sum(box==True))\n",
    "box.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(583, 743, 126)\n",
      "(583, 743)\n",
      "25868\n",
      "0 0\n",
      "1 0\n",
      "2 0\n",
      "3 0\n",
      "4 0\n",
      "5 0\n",
      "6 0\n",
      "7 0\n",
      "8 0\n",
      "9 0\n",
      "10 2617\n",
      "11 0\n",
      "12 0\n",
      "13 0\n",
      "14 0\n",
      "15 0\n",
      "16 0\n",
      "17 0\n",
      "18 0\n",
      "19 0\n",
      "20 0\n",
      "21 0\n",
      "22 0\n",
      "23 0\n",
      "24 0\n",
      "25 0\n",
      "26 0\n",
      "27 0\n",
      "28 2892\n",
      "29 0\n",
      "30 0\n",
      "31 0\n",
      "32 0\n",
      "33 0\n",
      "34 0\n",
      "35 0\n",
      "36 0\n",
      "37 0\n",
      "38 0\n",
      "39 0\n",
      "40 0\n",
      "41 0\n",
      "42 0\n",
      "43 0\n",
      "44 0\n",
      "45 0\n",
      "46 0\n",
      "47 0\n",
      "48 0\n",
      "49 0\n",
      "50 0\n",
      "51 0\n",
      "52 0\n",
      "53 0\n",
      "54 0\n",
      "55 2535\n",
      "56 0\n",
      "57 0\n",
      "58 0\n",
      "59 0\n",
      "60 0\n",
      "61 0\n",
      "62 0\n",
      "63 0\n",
      "64 0\n",
      "65 0\n",
      "66 0\n",
      "67 0\n",
      "68 0\n",
      "69 0\n",
      "70 0\n",
      "71 0\n",
      "72 0\n",
      "73 0\n",
      "74 0\n",
      "75 0\n",
      "76 0\n",
      "77 0\n",
      "78 0\n",
      "79 0\n",
      "80 0\n",
      "81 0\n",
      "82 0\n",
      "83 0\n",
      "84 0\n",
      "85 0\n",
      "86 0\n",
      "87 0\n",
      "88 0\n",
      "89 0\n",
      "90 0\n",
      "91 0\n",
      "92 0\n",
      "93 0\n",
      "94 0\n",
      "95 0\n",
      "96 0\n",
      "97 0\n",
      "98 0\n",
      "99 0\n",
      "100 0\n",
      "101 0\n",
      "102 0\n",
      "103 0\n",
      "104 0\n",
      "105 0\n",
      "106 0\n",
      "107 0\n",
      "108 0\n",
      "109 0\n",
      "110 0\n",
      "111 0\n",
      "112 0\n",
      "113 0\n",
      "114 0\n",
      "115 0\n",
      "116 0\n",
      "117 0\n",
      "118 0\n",
      "119 0\n",
      "120 0\n",
      "121 0\n",
      "122 0\n",
      "123 0\n",
      "124 0\n",
      "125 0\n"
     ]
    }
   ],
   "source": [
    "#idea: use data augmentation\n",
    "#      pretrain on amateur and fine tune on expert\n",
    "#      can try image segmentation first\n",
    "#      use a model from online source for video segmentation: challenge: no segmentation for all frames..? only 3 frame confirmed. \n",
    "\n",
    "label= np.array(train_data[63]['label'])\n",
    "print(label.shape)\n",
    "box= np.array(train_data[63]['box'])\n",
    "print(box.shape)\n",
    "print(np.sum(box))\n",
    "for i in range(label.shape[2]):\n",
    "    \n",
    "    print(i,np.sum(label[:,:,i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 420\n",
      "1 430\n",
      "2 319\n",
      "3 344\n",
      "4 319\n",
      "5 411\n",
      "6 298\n",
      "7 571\n",
      "8 300\n",
      "9 254\n",
      "10 453\n",
      "11 323\n",
      "12 376\n",
      "13 245\n",
      "14 339\n",
      "15 443\n",
      "16 393\n",
      "17 338\n",
      "18 568\n",
      "19 240\n",
      "20 450\n",
      "21 276\n",
      "22 382\n",
      "23 308\n",
      "24 312\n",
      "25 348\n",
      "26 240\n",
      "27 505\n",
      "28 539\n",
      "29 345\n",
      "30 427\n",
      "31 338\n",
      "32 282\n",
      "33 256\n",
      "34 280\n",
      "35 448\n",
      "36 335\n",
      "37 322\n",
      "38 389\n",
      "39 359\n",
      "40 293\n",
      "41 380\n",
      "42 320\n",
      "43 549\n",
      "44 494\n",
      "45 251\n",
      "46 8236\n",
      "47 6844\n",
      "48 10290\n",
      "49 8467\n",
      "50 10007\n",
      "51 16692\n",
      "52 9076\n",
      "53 7716\n",
      "54 11061\n",
      "55 9993\n",
      "56 10650\n",
      "57 7896\n",
      "58 6942\n",
      "59 8936\n",
      "60 7492\n",
      "61 6897\n",
      "62 10296\n",
      "63 8044\n",
      "64 7910\n"
     ]
    }
   ],
   "source": [
    "for i in range(65):\n",
    "    label= np.array(train_data[i]['label'])\n",
    "    print(i,np.sum(label[:,:,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make prediction for test\n",
    "predictions = []\n",
    "for d in test_data:\n",
    "    prediction = np.array(np.zeros_like(d['video']), dtype=np.bool)\n",
    "    height = prediction.shape[0]\n",
    "    width = prediction.shape[1]\n",
    "    prediction[int(height/2)-50:int(height/2+50), int(width/2)-50:int(width/2+50)] = True\n",
    "    \n",
    "    # DATA Strucure\n",
    "    predictions.append({\n",
    "        'name': d['name'],\n",
    "        'prediction': prediction\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save in correct format\n",
    "save_zipped_pickle(predictions, 'my_predictions.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pai_4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5 (default, Sep  4 2020, 07:30:14) \n[GCC 7.3.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "8a5edab282632443219e051e4ade2d1d5bbc671c781051bf1437897cbdfea0f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
